# Problem Statement

Object Counting, Handwriting Recognition, & Anomaly Detection in Acumen Chip Images

# Description

Each image represents a counting paper with a number of acumen chips on them. Using Computer Vision techniques, teams are expected to produce model(s) which perform the following:

1. Identify the number of chips present on each counting paper.
2. Identify the handwritten manual count present on each counting paper.
3. Indicate the location of anomalies with bounding boxes.

More details can be found on the [Kaggle Competition webpage](https://www.kaggle.com/c/nus-sds-dsc2021).

# Approach

For this challenge, we used [Mask R-CNN model](https://github.com/matterport/Mask_RCNN) for Object Detection tasks. We made use of the coordinates of the ROIs generated by its Region Proposal Network.

We used VGG Annotator 2.0 for labelling. The labels are encoded as follows:

- `normal`: 1, `double_stack`: 2, `black_spot`: 3, `handwriting`: 4

## Task 1: Identify the number of chips present on each counting paper

With the information from the bounding boxes, we count the number of normal (class_id = 1) + 2 \* double_stack (class_id = 2).

## Task 2: Identify the handwritten manual count present on each counting paper

We extract the ROI of the handwriting bounding boxes (if any), and then pass the sliced image for detection using `easy-ocr` function.

- If no handwriting region is detected, the count is -1.
- If more than 1 handwriting region is detected, the bounding box with the highest confidence score will be used.
  - If the returned value cannot be recognized as valid numerals, we will use the count from Task 1 as the answer.

## Task 3: Indicate the location of anomalies with bounding boxes

As the train set only contains very few `double_stack` and `black_spot` anomalies, we generated more images for each category for labelling.

- For `double_stack`, we simply sampled and cropped the normal chips from several images and overlaid them directly on another chip on Illustrator.
- For `black_spot` anomaly, we cropped out them out and used ImageDataGenerator to augment data in different sizes, orientation, brightness, etc, and then used Illustrator to superimpose them on selected images.
  - During the process, we also rotated the chip in 180 degrees as we knew that the black spot must be either at the top left hand corner or the bottom right hand corner.

During inference, we extract the ROI, coordinates and confidence scores of the anomalies bounding boxes (if any).

# Installation

1. Clone this repository.
   ```
   git clone https://github.com/derong97/acumen-chip-cv.git
   ```
2. Download pre-trained COCO weights (mask_rcnn_coco.h5) from the [releases page](https://github.com/matterport/Mask_RCNN/releases), or our pre-trained weights (mask_rcnn_acumen_final.h5) from [Google Cloud Storage Bucket](https://storage.googleapis.com/dsc_2021/mask_rcnn_acumen_final.h5).
3. Upload this folder to Google Drive. This is because we will be using Google Colaboratory and its GPU resources for training purposes.

# Train the model

Edit `train.ipynb` accordingly.

1. Change the path to your root directory.
   ```
   os.chdir('drive/My Drive/path/to/root/dir')
   ```
2. Change the paths to your own dataset, weights, and logs directory.
   ```
   python3 acumen_config.py train --dataset=/path/to/acumen/dataset --weights=/path/to/pretrained/weights --logs=/path/to/logs
   ```

Once done, run all the cells to start the training process. Make sure to change Runtime type to GPU.

When model training is completed, the weights can be found in the timestamp folder of your specified log directory.
Tip: monitor the `val_loss` during training, and use the weights that gives the lowest loss for inference.

# Infererence

Edit `inference.ipynb` accordingly.

1. Change the path to your root directory.
   ```
   os.chdir('drive/My Drive/path/to/root/dir')
   ```
2. Change the path to your trained weights.
   ```
   ACUMEN_WEIGHTS_PATH = ROOT_DIR + '/path/to/pretrained/weights'
   ```

Once done, run all the cells to start the inference process.

The results can be found in `submissions.csv`.

# Learning Points

1. Many chips could not be detected during the inference phase, affecting the total count. Especially because there were too many normal chips, when the RPNs were generated too close to each other, the non-max suppression threshold algorithm could have accidentally discarded the neighboring chip.
   - We could reduce the threshold value, or not label these majority chips at all (point 4).
   - Another alternative is to perform post-processing with the help of cv2 methods to see if there is indeed a missing chip.
2. Despite data augmentation, anomalies were not detected at all; some were mislabeled as normal. This could be because there were too few anomalies samples for the CNN to learn the features well.
   - More anomaly samples need to be generated and labelled.
3. The gradients exploded very quickly, resulting in nan loss. Lowering the learning rate did not help either. This problem appeared to happen when we started to feed in more data, but proper troubleshooting would require further exploration.
4. The handwriting region was detected very well. Unfortunately, EasyOCR was not very good at diciphering the texts. Neither was PyTesseract. Mapping possible confusing letters to numerals were helpful to a limited extent.
   - A more targeted neural network implementation (for digits only), or a combination of both EasyOCR and PyTesseract might improve the handwriting recognition.
   - Another possibility is to segment the numerals and then feed each of them into a simple VGG network trained on the MNIST dataset.
5. A better approach might be to label only the anomalies instead of every single chip for the Mask R-CNN model. This saves much manpower and time as well.
   - If we go ahead with this proposal, we would need to use cv2 techniques for Task 1.
   - Even though we experimented with several methods (e.g. thresholding, contrasting), the lighting conditions and varying resolutions of the given samples were too difficult for us to fine-tune the values properly. More advanced cv2 methods could be explored.
6. Another suggestion to improve the Mask-RCNN model is to pre-process the images (e.g. perform histogram equalization) before feeding into the network.
